{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9173786-ae5a-450d-94e7-fbc62e0be22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pyspark\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "from pyspark.sql.types import StructType, StructField, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a822582b-8a6e-4c0a-8601-0d0c94a1ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk:1.12.708,org.apache.hadoop:hadoop-aws:3.3.4 pyspark-shell'\n",
    "\n",
    "conf = SparkConf().set('spark.executor.extraJavaOptions','-Dcom.amazonaws.services.s3.enableV4=true'). \\\n",
    " set('spark.driver.extraJavaOptions','-Dcom.amazonaws.services.s3.enableV4=true'). \\\n",
    " setAppName('pyspark_aws').setMaster('local[*]')\n",
    "            \n",
    "sc=SparkContext(conf=conf)\n",
    "sc.setSystemProperty('com.amazonaws.services.s3.enableV4', 'true')\n",
    "\n",
    "accessKeyId='**************'\n",
    "secretAccessKey='****************************'\n",
    "\n",
    "hadoopConf = sc._jsc.hadoopConfiguration()\n",
    "hadoopConf.set('fs.s3a.access.key', accessKeyId)\n",
    "hadoopConf.set('fs.s3a.secret.key', secretAccessKey)\n",
    "hadoopConf.set('fs.s3a.endpoint', 's3-us-east-1.amazonaws.com')\n",
    "hadoopConf.set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "\n",
    "spark=SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b407f-53c0-4c72-b987-ffd65f588e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schema=StructType([StructField('Scorecard', StringType(), True), \n",
    "                        StructField('Team 1', StringType(), True), \n",
    "                        StructField('Team 2', StringType(), True), \n",
    "                        StructField('Winner', StringType(), True),\n",
    "                        StructField('Margin', StringType(), True), \n",
    "                        StructField('Ground', StringType(), True),\n",
    "                        StructField('Match Date', StringType(), True)\n",
    "                       ])\n",
    "\n",
    "df = spark.read.csv('../files/cricketMatchesDataset.csv', header=True)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e622b123-6df3-49d1-a20f-b20325902578",
   "metadata": {},
   "outputs": [],
   "source": [
    "india_df = df.select(col('Team 1'), \n",
    "                     col('Team 2'), \n",
    "                     col('Winner'), \n",
    "                     col('Ground'), \n",
    "                     col('Match Date')\n",
    "                    ).where((df[\"Team 1\"].contains(\"India\")) | (df[\"Team 2\"].contains(\"India\"))\n",
    "                           ).withColumn('id', monotonically_increasing_id()).cache()\n",
    "india_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ce493",
   "metadata": {},
   "outputs": [],
   "source": [
    "india_df.write.format('csv').option('header','true').save('path/to/bucket/filename.csv',mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
